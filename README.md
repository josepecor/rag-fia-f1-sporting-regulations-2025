# RAG FIA F1 Sporting Regulations 2025

## El problema

Este proyecto nace de una pregunta muy concreta: **Â¿cÃ³mo conseguir respuestas precisas cuando la informaciÃ³n estÃ¡ dispersa en cientos de pÃ¡ginas de documentos tÃ©cnicos complejos?**

Imagina buscar una regla especÃ­fica en el reglamento deportivo de FÃ³rmula 1 2025: mÃ¡s de 300 pÃ¡ginas donde una misma consulta puede requerir informaciÃ³n de mÃºltiples artÃ­culos, con referencias cruzadas, excepciones y casos especiales. **Leer el documento completo cada vez no es prÃ¡ctico. Confiar en la memoria de un modelo de IA puede llevar a respuestas inventadas.**

El desafÃ­o no era solo encontrar informaciÃ³n, sino **garantizar que cada respuesta fuera verificable y trazable** hasta su fuente original en las regulaciones oficiales.

**La soluciÃ³n: RAG (Retrieval-Augmented Generation)**

En lugar de depender Ãºnicamente de lo que un modelo "recuerda" de su entrenamiento, RAG combina dos capacidades:

1. **BÃºsqueda inteligente**: Recupera solo los fragmentos relevantes de los documentos oficiales
2. **GeneraciÃ³n contextual**: Utiliza esa informaciÃ³n verificada para construir respuestas precisas

El resultado es un sistema que **nunca inventa**. Cada afirmaciÃ³n estÃ¡ respaldada por el texto original de las regulaciones, con referencias directas a los artÃ­culos correspondientes. Si la informaciÃ³n no existe en los documentos, el sistema lo indica claramente en lugar de especular.

Este enfoque no solo resuelve el problema de la fiabilidad, sino que transforma 300+ pÃ¡ginas de regulaciones tÃ©cnicas en un asistente conversacional que responde en segundos con informaciÃ³n verificable y trazable.

---

## Los documentos utilizados

El sistema RAG se alimenta exclusivamente de documentaciÃ³n oficial.

- **PDF**: documento original, de la reglamentacion deportiva.
- **Markdown (.md)**: Su contenido son articulos densos con secciones y subsecciones.
- **Texto Plano (.txt)**: Su contenido son articulos menos densos, las secciones no contienen subsecciones.
- **CSV**: datos tabulares (puntos, clasificaciones, resultados).
- **JSON**: utilizado para realizar el Custionario de pregunta respuesta para realizar la evaluacion del modelo.
- **YAML**: utilizado para realizar el procesado de los datos en crudo que son los que fueron creados de forma manual.

---

## PreparaciÃ³n de los datos para el RAG

La preparaciÃ³n de los datos fue clave para el funcionamiento del sistema.

Primero, devido a la complegidad de automatizar la extraccion del contenido del pdf, a los formatos requeridos, se efectuo de forma manual, para ayudar al modelo se dividio por aticulos y apendices en documentos independientes, para facilitar que el contenido no estuviese mezclado. Cada fragmento contiene un conjunto de reglas relacionadas.

DespuÃ©s, se procesaron los ficheros en crudo, cambaindolos texto y agregando la informacion tabular a las secciones que lo requiriesen cuando se requiriese (esto es lo que se controla con el fichero previamente citado YAML).

---

## ElecciÃ³n del Modelo y Vector Database

### El Enfoque: ExperimentaciÃ³n sobre IntuiciÃ³n

En lugar de elegir un modelo "porque sÃ­", se diseÃ±o un experimento sistemÃ¡tico que evaluÃ³ **9 configuraciones diferentes**:

- 3 modelos de embeddings
- 3 estrategias de chunking (500, 1000, 2000 caracteres)
- EvaluaciÃ³n con 30+ queries reales

Cada configuraciÃ³n fue medida con mÃºltiples mÃ©tricas: F1 Score, NDCG, Precision@K, MAP, y MRR.

**Ventajas clave:**

- âœ… **Entrenado especÃ­ficamente para retrieval**, no solo similitud semÃ¡ntica general
- âœ… **Balance perfecto**: 384 dimensiones capturan la semÃ¡ntica sin sobrecarga
- âœ… **Consistencia**: Destaca especialmente en queries difÃ­ciles con mÃºltiples artÃ­culos
- âœ… **Eficiente**: 133MB, rÃ¡pido, y funciona en CPU

**Â¿Por quÃ© no usar modelos mÃ¡s grandes?**

1. El dominio es inglÃ©s tÃ©cnico (su especialidad)
2. La optimizaciÃ³n del chunking importa tanto como el modelo
3. Modelos pequeÃ±os bien optimizados > modelos grandes sin optimizar

### Estrategia de Chunking: 1000 caracteres con overlap de 200

**El experimento revelÃ³ un patrÃ³n claro:**

```
Chunk 500:   F1=0.798 â†’ Pierde contexto
Chunk 1000:  F1=0.847 â†’ Sweet spot âœ“
Chunk 2000:  F1=0.812 â†’ Demasiado genÃ©rico
```

**Â¿Por quÃ© 1000 funcionÃ³ mejor?**

Las regulaciones F1 tienen una estructura natural:

- PÃ¡rrafo principal (200-400 chars)
- Bullet points o sub-secciones (300-600 chars)
- Contexto adicional (100-200 chars)

**Total â‰ˆ 800-1000 caracteres** por concepto completo.

Chunks de 500 partÃ­an conceptos a la mitad. Chunks de 2000 mezclaban conceptos no relacionados. **1000 caracteres captura exactamente un concepto completo** con su contexto.

El overlap de 200 asegura que no perdamos informaciÃ³n en los "bordes" entre chunks.

### Vector Database: FAISS

La decisiÃ³n fue prÃ¡ctica, no ideolÃ³gica:

**Ventajas de FAISS:**

- âš¡ **Velocidad**: ~8ms por query
- ðŸ’° **Costo**: $0 (local) vs. servicios de pago ($X/mes)
- ðŸ”’ **Privacidad**: Datos completamente locales
- ðŸ“¦ **Simplicidad**: No requiere infraestructura adicional obligatoria (Docker, servidores)
- ðŸ’¾ **Eficiencia**: ~2.3 MB de Ã­ndice para 1,500 chunks
- ðŸš€ **Deploy**: Funciona en cualquier servidor ? Ordenador con pocos recursos

---

## EvaluaciÃ³n

Tras realizar test manuales, se encontro que ciertas preguntas contenian mas contexto del deseado incluyendo en la respuesta informaciÃ³n adicional que no esperaba.

a la pregunta de cuales eran los puntos y posiciones si no se finalizaba el 75% de la carrera, respondia correctamente, pero aportaba mas informaciÃ³n que solo las posiciones y puntuaciones, como que minimo se tenia que disputar 2 vueltras en bandera verde.

En otras preguntas directamente daba respuestas inclorrectas, si que tenia el contexto pero era completamente erronea.

a la pregunta puntos y si la carrera finalizaba de forma completa, respondia que solo puntan los 8 primeros cuando en la realidad son 10, es posible que no entienda la pregunta ya que la carrera sprint si que son los 8 primeros y eso le confunda, por hacer preguntas con poco contexto.

En esta primera versiÃ³n aun teniendo el mejor modelo del ensayo le falta mejorar.

## InicializaciÃ³n y Uso

Proyecto realizado con la versiÃ³n 3.11 de python, no se puede garantizar que con versiones inferiores funcione todas la librerias y garantizado que versiones posteriores librerias LangChain aun no son compatibles

```bash
# Crear entorno virtual con version especifica
python3.11 -m venv .venv

# Activar entono virtual en entorno Unix (Linux y Mac)
source ./.venv/bin/activate

# Instalar librerias necesarias
pip3 install -r requirements.txt
```

Procesar documentos crudos para crear el rag

```bash
python3 ./script/process_raw_documents.py
```

Creacion de los modelos RAG y evalueacion del mejor

```bash
python3 ./script/evaluate_rag_strategies.py
```

EjecuciÃ³n del chatbot en linea de comandos para test

```bash
# Modo Test: carga el mejor modelo guardado y ejecuta una consulta predefinida
python3 ./script/test_chatbot.py --mode test

# Modo Multiple: carga el mejor modelo guardado y ejecuta tres consultas predefinidas
python3 ./script/test_chatbot.py --mode multiple

# Modo Interactive: carga el mejor modelo guardado y ejecuta tipo consulta respuesta, es decir la consulta se puede hacer de forma personalizada
python3 ./script/test_chatbot.py --mode interactive
```

Ejecuta la interfaz de usuario grafica creada con streamlit

```bash
streamlit run ./scripts/app_chatbot.py
```
