# RAG FIA F1 Sporting Regulations 2025

## El problema

Este proyecto nace de una pregunta muy concreta: **¬øc√≥mo conseguir respuestas precisas cuando la informaci√≥n est√° dispersa en cientos de p√°ginas de documentos t√©cnicos complejos?**

Imagina buscar una regla espec√≠fica en el reglamento deportivo de F√≥rmula 1 2025: m√°s de 300 p√°ginas donde una misma consulta puede requerir informaci√≥n de m√∫ltiples art√≠culos, con referencias cruzadas, excepciones y casos especiales. **Leer el documento completo cada vez no es pr√°ctico. Confiar en la memoria de un modelo de IA puede llevar a respuestas inventadas.**

El desaf√≠o no era solo encontrar informaci√≥n, sino **garantizar que cada respuesta fuera verificable y trazable** hasta su fuente original en las regulaciones oficiales.

---

## La soluci√≥n: RAG (Retrieval-Augmented Generation)

En lugar de depender √∫nicamente de lo que un modelo "recuerda" de su entrenamiento, RAG combina dos capacidades:

1. **B√∫squeda inteligente**: Recupera solo los fragmentos relevantes de los documentos oficiales
2. **Generaci√≥n contextual**: Utiliza esa informaci√≥n verificada para construir respuestas precisas

El resultado es un sistema que **nunca inventa**. Cada afirmaci√≥n est√° respaldada por el texto original de las regulaciones, con referencias directas a los art√≠culos correspondientes. Si la informaci√≥n no existe en los documentos, el sistema lo indica claramente en lugar de especular.

Este enfoque no solo resuelve el problema de la fiabilidad, sino que transforma 300+ p√°ginas de regulaciones t√©cnicas en un asistente conversacional que responde en segundos con informaci√≥n verificable y trazable.

---

## Los documentos utilizados

El sistema RAG trabaja con distintos formatos documentales a lo largo de su pipeline.

No todos los formatos cumplen el mismo rol dentro del sistema: algunos forman parte de la base de conocimiento indexada y recuperable por el asistente, mientras que otros se utilizan como soporte para el procesado, estructuraci√≥n y evaluaci√≥n del sistema.

- **PDF**: documento original de la reglamentaci√≥n deportiva. Forma parte del conocimiento base del asistente.
- **Markdown (.md)**: art√≠culos densos con secciones y subsecciones, utilizados como conocimiento estructurado para el RAG.
- **Texto plano (.txt)**: art√≠culos menos densos sin subsecciones, utilizados como conocimiento complementario.
- **CSV**: datos tabulares (puntos, clasificaciones, resultados) empleados como informaci√≥n estructurada de apoyo.
- **JSON**: utilizado para construir el cuestionario de preguntas y respuestas empleado en la evaluaci√≥n del sistema.
- **YAML**: utilizado en el procesado de los datos en crudo, creados manualmente durante la fase de preparaci√≥n de la informaci√≥n.

---

## Preparaci√≥n de los datos para el RAG

La preparaci√≥n de los datos fue clave para el funcionamiento del sistema.

Primero, devido a la complegidad de automatizar la extraccion del contenido del pdf, a los formatos requeridos, se efectuo de forma manual, para ayudar al modelo se dividio por aticulos y apendices en documentos independientes, para facilitar que el contenido no estuviese mezclado. Cada fragmento contiene un conjunto de reglas relacionadas.

Despu√©s, se procesaron los ficheros en crudo, cambaindolos texto y agregando la informacion tabular a las secciones que lo requiriesen cuando se requiriese (esto es lo que se controla con el fichero previamente citado YAML).

---

## Elecci√≥n del Modelo y Vector Database

### El Enfoque: Experimentaci√≥n sobre Intuici√≥n

En lugar de elegir un modelo "porque s√≠", se dise√±o un experimento sistem√°tico que evalu√≥ **9 configuraciones diferentes**:

- 3 modelos de embeddings
- 3 estrategias de chunking (500, 1000, 2000 caracteres)
- Evaluaci√≥n con 30+ queries reales

Cada configuraci√≥n fue medida con m√∫ltiples m√©tricas: F1 Score, NDCG, Precision@K, MAP, y MRR.

**Ventajas clave:**

- ‚úÖ **Entrenado espec√≠ficamente para retrieval**, no solo similitud sem√°ntica general
- ‚úÖ **Balance perfecto**: 384 dimensiones capturan la sem√°ntica sin sobrecarga
- ‚úÖ **Consistencia**: Destaca especialmente en queries dif√≠ciles con m√∫ltiples art√≠culos
- ‚úÖ **Eficiente**: 133MB, r√°pido, y funciona en CPU

**¬øPor qu√© no usar modelos m√°s grandes?**

1. El dominio es ingl√©s t√©cnico (su especialidad)
2. La optimizaci√≥n del chunking importa tanto como el modelo
3. Modelos peque√±os bien optimizados > modelos grandes sin optimizar

### Estrategia de Chunking: 1000 caracteres con overlap de 200

**El experimento revel√≥ un patr√≥n claro:**

```
Chunk 500:   F1=0.798 ‚Üí Pierde contexto
Chunk 1000:  F1=0.847 ‚Üí Sweet spot ‚úì
Chunk 2000:  F1=0.812 ‚Üí Demasiado gen√©rico
```

**¬øPor qu√© 1000 funcion√≥ mejor?**

Las regulaciones F1 tienen una estructura natural:

- P√°rrafo principal (200-400 chars)
- Bullet points o sub-secciones (300-600 chars)
- Contexto adicional (100-200 chars)

**Total ‚âà 800-1000 caracteres** por concepto completo.

Chunks de 500 part√≠an conceptos a la mitad. Chunks de 2000 mezclaban conceptos no relacionados. **1000 caracteres captura exactamente un concepto completo** con su contexto.

El overlap de 200 asegura que no perdamos informaci√≥n en los "bordes" entre chunks.

### Vector Database: FAISS

La decisi√≥n fue pr√°ctica, no ideol√≥gica:

**Ventajas de FAISS:**

- ‚ö° **Velocidad**: ~8ms por query
- üí∞ **Costo**: $0 (local) vs. servicios de pago ($X/mes)
- üîí **Privacidad**: Datos completamente locales
- üì¶ **Simplicidad**: No requiere infraestructura adicional obligatoria (Docker, servidores)
- üíæ **Eficiencia**: ~2.3 MB de √≠ndice para 1,500 chunks
- üöÄ **Deploy**: Funciona en cualquier servidor ? Ordenador con pocos recursos

---

## Evaluaci√≥n

Tras realizar test manuales, se encontro que ciertas preguntas contenian mas contexto del deseado incluyendo en la respuesta informaci√≥n adicional que no esperaba.

A la pregunta de cuales eran los puntos y posiciones si no se finalizaba el 75% de la carrera, respondia correctamente, pero aportaba mas informaci√≥n que solo las posiciones y puntuaciones, como que minimo se tenia que disputar 2 vueltras en bandera verde.

En otras preguntas directamente daba respuestas inclorrectas, si que tenia el contexto pero era completamente erronea.

A la pregunta puntos y si la carrera finalizaba de forma completa, respondia que solo puntan los 8 primeros cuando en la realidad son 10, es posible que no entienda la pregunta ya que la carrera sprint si que son los 8 primeros y eso le confunda, por hacer preguntas con poco contexto.

En esta primera versi√≥n aun teniendo el mejor modelo del ensayo le falta mejorar.

---

## Mejoras

Como parte del an√°lisis de posibles mejoras del sistema RAG, se exploraron distintas estrategias orientadas a incrementar la precisi√≥n y coherencia de las respuestas generadas, as√≠ como a reducir errores derivados de la recuperaci√≥n de contexto.

Una de las aproximaciones evaluadas consisti√≥ en modificar la estrategia de preprocesado documental, dividiendo cada art√≠culo en secciones independientes antes de su indexaci√≥n. La hip√≥tesis inicial era que una segmentaci√≥n m√°s fina permitir√≠a recuperar fragmentos m√°s espec√≠ficos y directamente relacionados con la consulta del usuario.

Sin embargo, los resultados obtenidos mostraron respuestas m√°s incompletas y con menor coherencia global, debido principalmente a la p√©rdida de contexto entre secciones relacionadas del mismo art√≠culo. Esta fragmentaci√≥n excesiva dificult√≥ la reconstrucci√≥n del significado completo de ciertas normas, por lo que la estrategia fue descartada tras su evaluaci√≥n, manteni√©ndose una segmentaci√≥n que preserva mayor continuidad sem√°ntica entre fragmentos.

Como complemento al sistema base, se incorpor√≥ una fase de postprocesado de las respuestas orientada a mejorar su legibilidad y naturalidad, sin alterar el contenido factual recuperado. Esta mejora tiene un impacto principalmente en la experiencia de uso del asistente, pero no modifica el proceso de recuperaci√≥n ni la exactitud de la informaci√≥n proporcionada.

Finalmente, se identifican como l√≠neas futuras de mejora la especializaci√≥n del sistema en m√∫ltiples modelos orientados a distintas categor√≠as normativas, aprovechando la clasificaci√≥n existente en los ficheros YAML. Esta aproximaci√≥n permitir√≠a reducir el espacio de b√∫squeda de cada modelo y, mediante un mecanismo de orquestaci√≥n de intenciones, construir un sistema m√°s escalable y potencialmente m√°s preciso, donde cada componente se especialice en un subconjunto concreto de la normativa.

---

## Conlusiones

La primera lecci√≥n extra√≠da de este proyecto es que un sistema RAG y un modelo LLM no representan la misma arquitectura de interpretaci√≥n del lenguaje natural, aunque pueden complementarse. Mientras que un LLM opera principalmente sobre el conocimiento impl√≠cito adquirido durante su entrenamiento, un sistema RAG fundamenta sus respuestas en informaci√≥n externa recuperada din√°micamente, lo que resulta clave en contextos donde la trazabilidad y la fidelidad a las fuentes son requisitos esenciales.

Otro aspecto fundamental identificado es la importancia del procesado y la estructuraci√≥n previa de la informaci√≥n incorporada al sistema RAG. La forma en que se agrupa o segmenta el contenido antes de su indexaci√≥n tiene un impacto directo en la calidad de las respuestas. En particular, par√°metros como el tama√±o de los fragmentos de texto y el grado de solapamiento entre ellos resultan determinantes para lograr un equilibrio adecuado entre cobertura contextual y precisi√≥n sem√°ntica, como se ha observado al evaluar distintas estrategias de segmentaci√≥n documental.

Asimismo, el modelo base empleado act√∫a como traductor de intenciones entre la consulta del usuario y el conocimiento recuperado. La elecci√≥n de este modelo condiciona aspectos como el idioma de entrada y la correcta interpretaci√≥n de las preguntas planteadas. Si el modelo no est√° alineado con el lenguaje utilizado en las consultas, la recuperaci√≥n de informaci√≥n y la posterior construcci√≥n de respuestas se ven inevitablemente afectadas, independientemente de la calidad del sistema RAG subyacente.

En relaci√≥n con el prompting, el proyecto ha puesto de manifiesto que los ajustes realizados a nivel del prompt conversacional del chatbot tienen un impacto limitado sobre la precisi√≥n factual de las respuestas en un sistema RAG. Este resultado refuerza la idea de que, en arquitecturas basadas en recuperaci√≥n de contexto, la calidad del sistema depende principalmente de la fase de recuperaci√≥n, del preprocesado de los documentos y de los criterios utilizados para seleccionar y presentar la evidencia, m√°s que del estilo conversacional del asistente.

Finalmente, se observa que concentrar grandes vol√∫menes de informaci√≥n heterog√©nea en un √∫nico modelo puede incrementar el riesgo de respuestas ambiguas o incompletas cuando existen contenidos similares. Como l√≠nea futura de mejora, la segmentaci√≥n del conocimiento en modelos especializados por categor√≠a normativa, junto con un mecanismo de orquestaci√≥n de intenciones, se presenta como una estrategia prometedora para reducir errores y mejorar la precisi√≥n global del sistema.

## Inicializaci√≥n y Uso

Proyecto realizado con la versi√≥n 3.11 de python, no se puede garantizar que con versiones inferiores funcione todas la librerias y garantizado que versiones posteriores librerias LangChain aun no son compatibles

```bash
# Crear entorno virtual con version especifica
python3.11 -m venv .venv

# Activar entono virtual en entorno Unix (Linux y Mac)
source ./.venv/bin/activate

# Instalar librerias necesarias
pip3 install -r requirements.txt
```

Procesar documentos crudos para crear el rag

```bash
python3 ./script/process_raw_documents.py
```

Creacion de los modelos RAG y evalueacion del mejor

```bash
python3 ./script/evaluate_rag_strategies.py
```

Ejecuci√≥n del chatbot en linea de comandos para test

```bash
# Modo Test: carga el mejor modelo guardado y ejecuta una consulta predefinida
python3 ./script/test_chatbot.py --mode test

# Modo Multiple: carga el mejor modelo guardado y ejecuta tres consultas predefinidas
python3 ./script/test_chatbot.py --mode multiple

# Modo Interactive: carga el mejor modelo guardado y ejecuta tipo consulta respuesta, es decir la consulta se puede hacer de forma personalizada
python3 ./script/test_chatbot.py --mode interactive

# Modo Compare: dada una consulta debuelve el resultado sin tratat y tratado con el system prompt y algo mas de codigo
python3 ./script/test_chatbot.py --mode compare
```

Ejecuta la interfaz de usuario grafica creada con streamlit

```bash
streamlit run ./scripts/app_chatbot.py
```
